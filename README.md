Heart Disease Prediction ğŸ«€
ğŸ“Œ Overview

This project predicts the likelihood of heart disease using Logistic Regression and K-Nearest Neighbors (KNN).
The dataset includes medical attributes such as age, cholesterol, blood pressure, chest pain type, and more.

The objective is to demonstrate how machine learning can support early detection and preventive healthcare.

ğŸ“Š Dataset

Source: UCI Heart Disease Dataset

Target variable:

0 â†’ No Heart Disease

1 â†’ Heart Disease Present

Features used:

Age â€“ Age of the patient

Sex â€“ Gender (1 = male, 0 = female)

Chest pain type â€“ Type of chest pain

BP â€“ Resting blood pressure

Cholesterol â€“ Serum cholesterol

FBS over 120 â€“ Fasting blood sugar > 120 mg/dl

EKG results â€“ Resting electrocardiographic results

Max HR â€“ Maximum heart rate achieved

Exercise angina â€“ Exercise induced angina

ST depression â€“ Depression induced by exercise

Slope of ST â€“ Slope of the ST segment

Number of vessels fluro â€“ Number of major vessels colored by fluoroscopy

Thallium â€“ Thalassemia (3 = normal, 6 = fixed defect, 7 = reversible defect)

âš™ï¸ Project Workflow

Data Preprocessing

Encoded categorical features

Normalized numeric features

Train-test split

Exploratory Data Analysis (EDA)

Correlation heatmap

Feature-target relationships

Models Used

Logistic Regression

K-Nearest Neighbors (KNN)

Evaluation Metrics

Accuracy Score

Precision, Recall, F1-score

Confusion Matrix

ğŸ› ï¸ Tech Stack

Language: Python

Libraries: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn

Dependency Management: Poetry

ğŸš€ Installation
Option 1: Using requirements.txt
git clone https://github.com/BhagyaSreeV29/Heart_Prediction.git
cd Heart_Prediction
pip install -r requirements.txt

Option 2: Using Poetry
git clone https://github.com/BhagyaSreeV29/Heart_Prediction.git
cd Heart_Prediction
poetry install

â–¶ï¸ Running the Notebook
jupyter notebook heartpred.ipynb

ğŸ“ˆ Results
Model	Accuracy	Precision (0 / 1)	Recall (0 / 1)	F1-score (0 / 1)
K-Nearest Neighbors	0.78	0.78 / 0.79	0.86 / 0.68	0.82 / 0.73
Logistic Regression	0.88	0.87 / 0.90	0.93 / 0.82	0.90 / 0.86
